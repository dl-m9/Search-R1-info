#!/usr/bin/env python3
"""
Test script for SePer integration with Search-R1
Tests the core functionality of SePer reward calculation
"""

import sys
import os
import torch
import json
from typing import Dict, Any

# Add paths
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(project_root, 'verl', 'utils', 'reward_score'))
sys.path.insert(0, os.path.join(project_root, 'seper'))

def test_seper_import():
    """Test SePer module imports"""
    print("=" * 60)
    print("Testing SePer Module Imports...")

    try:
        from seper.calculate import gen_answers_batch, calculate_uncertainty_soft_batch
        from seper.models.huggingface_models import HuggingfaceModel
        from seper.uncertainty_measures.semantic_entropy import EntailmentDeberta
        print("‚úÖ SePer modules imported successfully")
        return True
    except ImportError as e:
        print(f"‚ùå SePer import failed: {e}")
        return False

def test_seper_reward_import():
    """Test SePer reward module import"""
    print("=" * 60)
    print("Testing SePer Reward Module Import...")

    try:
        from verl.utils.reward_score.seper_reward import SePerRewardCalculator, compute_seper_reward
        print("‚úÖ SePer reward module imported successfully")
        return True
    except ImportError as e:
        print(f"‚ùå SePer reward import failed: {e}")
        return False

def test_seper_calculator_initialization():
    """Test SePer calculator initialization"""
    print("=" * 60)
    print("Testing SePer Calculator Initialization...")

    try:
        from verl.utils.reward_score.seper_reward import SePerRewardCalculator

        # Test with disabled mode (no models needed)
        calculator = SePerRewardCalculator(enabled=False)
        print("‚úÖ SePer calculator (disabled mode) initialized")

        # Test with auto model detection
        calculator = SePerRewardCalculator(
            model_path=None,  # Should auto-detect
            num_generations=2,  # Small for testing
            enabled=True
        )
        print("‚úÖ SePer calculator (auto model) initialized")
        print(f"   Model path: {calculator.model_path}")
        return calculator
    except Exception as e:
        print(f"‚ùå SePer calculator initialization failed: {e}")
        return None

def test_question_context_extraction():
    """Test question and context extraction from Search-R1 format"""
    print("=" * 60)
    print("Testing Question/Context Extraction...")

    try:
        from verl.utils.reward_score.seper_reward import SePerRewardCalculator
        calculator = SePerRewardCalculator(enabled=False)

        # Test cases
        test_cases = [
            {
                'name': 'Basic Q&A with search',
                'text': '''Answer the given question.
Question: Is Elon Musk older than Sam Altman?
<reasoning>I need to compare their ages.</reasoning>
<search>Elon Musk birth date Sam Altman birth date</search>
<information>Elon Musk was born on June 28, 1971. Sam Altman was born on April 22, 1985.</information>
<answer>Yes, Elon Musk is older than Sam Altman.</answer>''',
                'expected_question': 'Is Elon Musk older than Sam Altman?',
                'expected_context': 'Elon Musk was born on June 28, 1971. Sam Altman was born on April 22, 1985.'
            },
            {
                'name': 'No search context',
                'text': '''Answer the given question.
Question: What is the capital of France?
<answer>Paris</answer>''',
                'expected_question': 'What is the capital of France?',
                'expected_context': ''
            },
            {
                'name': 'Multiple search results',
                'text': '''Answer the given question.
Question: Who invented the telephone?
<search>telephone invention</search>
<information>Alexander Graham Bell invented the telephone.</information>
<search>telephone patent</search>
<information>Bell filed for a telephone patent in 1876.</information>
<answer>Alexander Graham Bell invented the telephone.</answer>''',
                'expected_question': 'Who invented the telephone?',
                'expected_context': 'Alexander Graham Bell invented the telephone.\nBell filed for a telephone patent in 1876.'
            }
        ]

        for i, test_case in enumerate(test_cases, 1):
            print(f"\n--- Test Case {i}: {test_case['name']} ---")
            question, context = calculator.extract_question_and_context(test_case['text'])

            print(f"Question: '{question}'")
            print(f"Context: '{context}'")

            # Check results
            question_ok = question.strip() == test_case['expected_question'].strip()
            context_ok = context.strip() == test_case['expected_context'].strip()

            print(f"‚úÖ Question correct: {question_ok}")
            print(f"‚úÖ Context correct: {context_ok}")

            if not (question_ok and context_ok):
                print("‚ùå Extraction failed!")
                print(f"Expected question: '{test_case['expected_question']}'")
                print(f"Expected context: '{test_case['expected_context']}'")

        return True

    except Exception as e:
        print(f"‚ùå Question/context extraction test failed: {e}")
        return False

def test_reward_calculation():
    """Test reward calculation with sample data"""
    print("=" * 60)
    print("Testing Reward Calculation...")

    try:
        from verl.utils.reward_score.seper_reward import compute_seper_reward

        # Mock sample data
        sample_solution = '''Answer the given question.
Question: Is Paris the capital of France?
<reasoning>I need to verify if Paris is the capital of France.</reasoning>
<search>capital of France</search>
<information>Paris is the capital and most populous city of France.</information>
<answer>Yes, Paris is the capital of France.</answer>'''

        sample_ground_truth = {
            'target': ['Yes']
        }

        print("Testing with disabled SePer...")
        # Test with SePer disabled
        os.environ['SEPER_ENABLED'] = 'false'
        reward = compute_seper_reward(sample_solution, sample_ground_truth)
        print(f"Disabled SePer reward: {reward}")

        if reward == 0.0:
            print("‚úÖ Disabled SePer test passed")
        else:
            print("‚ùå Disabled SePer test failed")

        print("\nTesting with enabled SePer (would run full computation)...")
        print("Note: Full SePer test requires model loading and is computationally intensive")
        print("‚úÖ Basic reward function structure verified")

        return True

    except Exception as e:
        print(f"‚ùå Reward calculation test failed: {e}")
        return False

def test_enhanced_reward_manager():
    """Test enhanced reward manager"""
    print("=" * 60)
    print("Testing Enhanced Reward Manager...")

    try:
        from verl.trainer.main_ppo_seper import EnhancedRewardManager
        from transformers import AutoTokenizer

        # Mock tokenizer for testing
        tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')

        # Test with SePer disabled
        reward_manager = EnhancedRewardManager(
            tokenizer=tokenizer,
            seper_weight=0.0,
            seper_config={'enabled': False}
        )
        print("‚úÖ Enhanced reward manager (SePer disabled) initialized")

        # Test with SePer enabled
        reward_manager = EnhancedRewardManager(
            tokenizer=tokenizer,
            seper_weight=0.5,
            seper_config={'enabled': True, 'num_generations': 2}
        )
        print("‚úÖ Enhanced reward manager (SePer enabled) initialized")

        return True

    except Exception as e:
        print(f"‚ùå Enhanced reward manager test failed: {e}")
        return False

def test_config_parsing():
    """Test configuration parsing"""
    print("=" * 60)
    print("Testing Configuration Parsing...")

    try:
        from verl.trainer.main_ppo_seper import _select_rm_score_fn

        # Test data source selection
        test_sources = [
            ('nq_search', True),
            ('hotpotqa_search', True),
            ('triviaqa_search', True),
            ('other_dataset', False)
        ]

        for source, expected_seper in test_sources:
            score_fn = _select_rm_score_fn(source)
            has_seper = 'seper' in score_fn.__name__ if hasattr(score_fn, '__name__') else False

            print(f"Source: {source} -> SePer enabled: {has_seper} (expected: {expected_seper})")

            if has_seper == expected_seper:
                print(f"‚úÖ {source} configuration correct")
            else:
                print(f"‚ùå {source} configuration incorrect")

        return True

    except Exception as e:
        print(f"‚ùå Configuration parsing test failed: {e}")
        return False

def run_integration_tests():
    """Run all integration tests"""
    print("üöÄ Starting SePer Integration Tests...")
    print("This test suite validates the SePer integration with Search-R1")

    tests = [
        ("Module Imports", test_seper_import),
        ("SePer Reward Import", test_seper_reward_import),
        ("SePer Calculator Initialization", test_seper_calculator_initialization),
        ("Question/Context Extraction", test_question_context_extraction),
        ("Reward Calculation", test_reward_calculation),
        ("Enhanced Reward Manager", test_enhanced_reward_manager),
        ("Configuration Parsing", test_config_parsing),
    ]

    results = {}

    for test_name, test_func in tests:
        try:
            result = test_func()
            results[test_name] = result
        except Exception as e:
            print(f"‚ùå Test {test_name} crashed: {e}")
            results[test_name] = False

    # Summary
    print("\n" + "=" * 60)
    print("üèÅ TEST SUMMARY")
    print("=" * 60)

    passed = 0
    total = len(results)

    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name}: {status}")
        if result:
            passed += 1

    print(f"\nOverall: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All tests passed! SePer integration is ready for use.")
    else:
        print("‚ö†Ô∏è  Some tests failed. Check the errors above.")

    return passed == total

if __name__ == "__main__":
    success = run_integration_tests()
    sys.exit(0 if success else 1)