{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74fcab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                           question  \\\n",
      "0  test_0          who got the first nobel prize in physics?   \n",
      "1  test_1    when is the next deadpool movie being released?   \n",
      "2  test_2  which mode is used for short wave broadcast se...   \n",
      "3  test_3  the south west wind blows across nigeria between?   \n",
      "4  test_4                what does hp mean in war and order?   \n",
      "\n",
      "                  golden_answers data_source  \\\n",
      "0       [Wilhelm Conrad Röntgen]          nq   \n",
      "1                 [May 18, 2018]          nq   \n",
      "2                 [Olivia, MFSK]          nq   \n",
      "3               [till September]          nq   \n",
      "4  [hit points or health points]          nq   \n",
      "\n",
      "                                              prompt         ability  \\\n",
      "0  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "1  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "2  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "3  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "4  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "\n",
      "                                        reward_model  \\\n",
      "0  {'ground_truth': {'target': ['Wilhelm Conrad R...   \n",
      "1  {'ground_truth': {'target': ['May 18, 2018']},...   \n",
      "2  {'ground_truth': {'target': ['Olivia', 'MFSK']...   \n",
      "3  {'ground_truth': {'target': ['till September']...   \n",
      "4  {'ground_truth': {'target': ['hit points or he...   \n",
      "\n",
      "                      extra_info  \n",
      "0  {'index': 0, 'split': 'test'}  \n",
      "1  {'index': 1, 'split': 'test'}  \n",
      "2  {'index': 2, 'split': 'test'}  \n",
      "3  {'index': 3, 'split': 'test'}  \n",
      "4  {'index': 4, 'split': 'test'}  \n",
      "****************************************************************************************************\n",
      "       data_source                                             prompt  \\\n",
      "0  2wikimultihopqa  [{'content': 'Answer the given question. You m...   \n",
      "1  2wikimultihopqa  [{'content': 'Answer the given question. You m...   \n",
      "2  2wikimultihopqa  [{'content': 'Answer the given question. You m...   \n",
      "3  2wikimultihopqa  [{'content': 'Answer the given question. You m...   \n",
      "4  2wikimultihopqa  [{'content': 'Answer the given question. You m...   \n",
      "\n",
      "          ability                                       reward_model  \\\n",
      "0  fact-reasoning  {'ground_truth': {'target': ['Crown Prince Hyo...   \n",
      "1  fact-reasoning  {'ground_truth': {'target': ['Grouplogic']}, '...   \n",
      "2  fact-reasoning  {'ground_truth': {'target': ['Hollywood']}, 's...   \n",
      "3  fact-reasoning  {'ground_truth': {'target': ['Chumki Chowdhury...   \n",
      "4  fact-reasoning  {'ground_truth': {'target': ['The Devil'S Mine...   \n",
      "\n",
      "                      extra_info  \n",
      "0  {'index': 0, 'split': 'test'}  \n",
      "1  {'index': 1, 'split': 'test'}  \n",
      "2  {'index': 2, 'split': 'test'}  \n",
      "3  {'index': 3, 'split': 'test'}  \n",
      "4  {'index': 4, 'split': 'test'}  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取文件\n",
    "df = pd.read_parquet('/forest/forest/Search-R1-info/data/nq_search/test.parquet')\n",
    "# 查看前5行数据\n",
    "print(df.head())\n",
    "# 查看数据基本信息\n",
    "# print(df.info())\n",
    "\n",
    "print(\"*\"*100)\n",
    "\n",
    "df = pd.read_parquet('/forest/forest/Search-R1-info/data/test_data/test.parquet')\n",
    "# 查看前5行数据\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c025e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/searchr1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 169615 examples [00:01, 95177.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 169615\n",
      "\n",
      "Data source distribution:\n",
      "data_source\n",
      "hotpotqa    90447\n",
      "nq          79168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HotpotQA count: 90447\n",
      "NQ count: 79168\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# 读取文件\n",
    "file_path = '/forest/forest/Search-R1-info/data/nq_hotpotqa_train/train.parquet'\n",
    "ds = datasets.load_dataset('parquet', data_files=file_path)\n",
    "\n",
    "# 获取数据集（通常是 'train' split）\n",
    "split_name = list(ds.keys())[0]\n",
    "df = ds[split_name].to_pandas()\n",
    "\n",
    "print(f'Total examples: {len(df)}')·\n",
    "print(f'\\nData source distribution:')\n",
    "print(df['data_source'].value_counts())\n",
    "print(f'\\nHotpotQA count: {len(df[df[\"data_source\"] == \"hotpotqa\"])}')\n",
    "print(f'NQ count: {len(df[df[\"data_source\"] == \"nq\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9317bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted HotpotQA examples: 90447\n",
      "\n",
      "First few rows:\n",
      "            id                                           question  \\\n",
      "79168  train_0  Which magazine was started first Arthur's Maga...   \n",
      "79169  train_1  The Oberoi family is part of a hotel company t...   \n",
      "79170  train_2  Musician and satirist Allie Goertz wrote a son...   \n",
      "79171  train_3    What nationality was James Henry Miller's wife?   \n",
      "79172  train_4  Cadmium Chloride is slightly soluble in this c...   \n",
      "\n",
      "                  golden_answers data_source  \\\n",
      "79168        [Arthur's Magazine]    hotpotqa   \n",
      "79169                    [Delhi]    hotpotqa   \n",
      "79170  [President Richard Nixon]    hotpotqa   \n",
      "79171                 [American]    hotpotqa   \n",
      "79172                  [alcohol]    hotpotqa   \n",
      "\n",
      "                                                  prompt         ability  \\\n",
      "79168  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "79169  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "79170  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "79171  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "79172  [{'content': 'Answer the given question. You m...  fact-reasoning   \n",
      "\n",
      "                                            reward_model  \\\n",
      "79168  {'ground_truth': {'target': ['Arthur's Magazin...   \n",
      "79169  {'ground_truth': {'target': ['Delhi']}, 'style...   \n",
      "79170  {'ground_truth': {'target': ['President Richar...   \n",
      "79171  {'ground_truth': {'target': ['American']}, 'st...   \n",
      "79172  {'ground_truth': {'target': ['alcohol']}, 'sty...   \n",
      "\n",
      "                           extra_info  \\\n",
      "79168  {'index': 0, 'split': 'train'}   \n",
      "79169  {'index': 1, 'split': 'train'}   \n",
      "79170  {'index': 2, 'split': 'train'}   \n",
      "79171  {'index': 3, 'split': 'train'}   \n",
      "79172  {'index': 4, 'split': 'train'}   \n",
      "\n",
      "                                                metadata  \n",
      "79168  {'type': 'comparison', 'level': 'medium', 'sup...  \n",
      "79169  {'type': 'bridge', 'level': 'medium', 'support...  \n",
      "79170  {'type': 'bridge', 'level': 'hard', 'supportin...  \n",
      "79171  {'type': 'bridge', 'level': 'medium', 'support...  \n",
      "79172  {'type': 'bridge', 'level': 'medium', 'support...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:02<00:00,  2.67ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to: /forest/forest/Search-R1-info/data/my_train_hotpotqa.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 提取 hotpotqa 数据\n",
    "hotpotqa_df = df[df['data_source'] == 'hotpotqa'].copy()\n",
    "\n",
    "print(f'Extracted HotpotQA examples: {len(hotpotqa_df)}')\n",
    "print(f'\\nFirst few rows:')\n",
    "print(hotpotqa_df.head())\n",
    "\n",
    "# 保存为 parquet 文件\n",
    "output_path = '/forest/forest/Search-R1-info/data/my_train_hotpotqa.parquet'\n",
    "import os\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# 使用 datasets 库保存（保持格式一致）\n",
    "hotpotqa_dataset = datasets.Dataset.from_pandas(hotpotqa_df)\n",
    "hotpotqa_dataset.to_parquet(output_path)\n",
    "\n",
    "print(f'\\nSaved to: {output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f24cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /forest/forest/Search-R1-info/logs/test-11030120-continue-nq-search-r1-grpo-qwen2.5-3b-infogain.log\n",
      "Total occurrences of \"info_gain_score\": 47616\n"
     ]
    }
   ],
   "source": [
    "# 统计日志文件中 \"info_gain_score\" 的出现次数\n",
    "log_file = '/forest/forest/Search-R1-info/logs/test-11030120-continue-nq-search-r1-grpo-qwen2.5-3b-infogain.log'\n",
    "\n",
    "# 读取日志文件并统计\n",
    "with open(log_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    count = content.count('info_gain_score')\n",
    "\n",
    "print(f'File: {log_file}')\n",
    "print(f'Total occurrences of \"info_gain_score\": {count}')\n",
    "\n",
    "# 如果需要查看包含该字符串的行，可以取消下面的注释\n",
    "# lines_with_keyword = [line for line in content.split('\\n') if 'info_gain_score' in line]\n",
    "# print(f'\\nLines containing \"info_gain_score\": {len(lines_with_keyword)}')\n",
    "# if lines_with_keyword:\n",
    "#     print('\\nFirst 5 lines:')\n",
    "#     for i, line in enumerate(lines_with_keyword[:5]):\n",
    "#         print(f'{i+1}. {line[:200]}...' if len(line) > 200 else f'{i+1}. {line}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84dc599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/searchr1/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/searchr1/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取 wandb 运行数据并提取 val/test_score/* 指标...\n",
      "\n",
      "尝试使用 wandb API 读取历史数据...\n",
      "成功从 project 'Search-R1' 读取运行数据\n",
      "历史数据形状: (229, 74)\n",
      "历史数据列: ['timing_s/ref', 'critic/advantages/max', 'state_tokens/coverage', 'critic/rewards/max', 'timing_s/step', 'critic/advantages/mean', 'critic/score/min', '_step', 'env/ratio_of_valid_action', 'reward/output_score_max', 'critic/returns/min', 'reward/output_score_mean', 'critic/score/mean', 'timing_per_token_ms/adv', 'prompt_length/min', 'timing_s/adv', 'actor/kl_coef', 'response_length/mean', 'global_seqlen/balanced_max', 'prompt_length/mean', 'critic/rewards/mean', 'reward/all_score_mean', '_runtime', 'timing_s/update_actor', 'timing_s/gen', 'actor/ppo_kl', 'actor/entropy_loss', 'global_seqlen/minmax_diff', 'timing_per_token_ms/ref', 'actor/pg_clipfrac', 'mfu/actor', 'env/number_of_actions/max', 'env/finish_ratio', 'critic/advantages/min', 'env/number_of_valid_search', 'env/number_of_valid_action', 'reward/info_gain_score_max', 'actor/pg_loss', 'reward/output_score_min', 'global_seqlen/balanced_min', 'reward/all_score_max', 'critic/rewards/min', 'state_tokens/total', 'critic/returns/mean', 'critic/returns/max', 'global_seqlen/mean', 'response_length/min', 'env/number_of_actions/mean', 'prompt_length/clip_ratio', 'reward/info_gain_score_min', 'env/number_of_actions/min', 'reward/info_gain_score_mean', 'prompt_length/max', 'response_length/clip_ratio', 'reward/all_score_min', 'global_seqlen/max', 'timing_per_token_ms/update_actor', 'actor/lr', 'critic/score/max', 'timing_per_token_ms/gen', 'actor/kl_loss', 'response_length/max', '_timestamp', 'global_seqlen/min', 'actor/grad_norm', 'val/test_score/2wikimultihopqa', 'timing_s/save_checkpoint', 'val/test_score/nq', 'val/test_score/popqa', 'timing_s/testing', 'val/test_score/bamboogle', 'val/test_score/hotpotqa', 'val/test_score/musique', 'val/test_score/triviaqa']\n",
      "\n",
      "找到的 val/test_score 列: ['val/test_score/2wikimultihopqa', 'val/test_score/nq', 'val/test_score/popqa', 'val/test_score/bamboogle', 'val/test_score/hotpotqa', 'val/test_score/musique', 'val/test_score/triviaqa']\n",
      "\n",
      "尝试方法2：使用 wandb 命令行工具...\n",
      "\n",
      "尝试方法3：直接读取 wandb 历史文件...\n",
      "方法3失败: cannot import name 'FileStream' from 'wandb.sdk.internal.file_stream' (/opt/conda/envs/searchr1/lib/python3.9/site-packages/wandb/sdk/internal/file_stream.py)\n",
      "将使用日志文件方法...\n",
      "\n",
      "方法4：从日志文件中提取 val/test_score 数据...\n",
      "找到日志文件: 4 个\n",
      "正在读取: output.log\n",
      "正在读取: debug-internal.log\n",
      "正在读取: debug.log\n",
      "正在读取: debug-core.log\n",
      "\n",
      "使用从 wandb API 读取的数据\n",
      "\n",
      "================================================================================\n",
      "每一轮所有 data_source 的平均值:\n",
      "================================================================================\n",
      "   round  average_score\n",
      "step_100       0.314957\n",
      "step_120       0.309734\n",
      "step_140       0.314140\n",
      "step_160       0.316552\n",
      "step_180       0.315200\n",
      " step_20       0.310142\n",
      "step_200       0.311778\n",
      "step_220       0.315324\n",
      " step_40       0.308957\n",
      " step_60       0.314145\n",
      " step_80       0.313526\n",
      "\n",
      "所有轮次平均值的总体平均: 0.3131\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 设置 wandb 运行目录\n",
    "wandb_run_dir = '/forest/forest/Search-R1-info/wandb/run-20251109_165031-h8hpdlth'\n",
    "\n",
    "print(\"正在读取 wandb 运行数据并提取 val/test_score/* 指标...\\n\")\n",
    "\n",
    "# 方法：使用 wandb API 读取历史数据\n",
    "val_test_scores_by_round = {}  # {round: {data_source: [scores]}}\n",
    "\n",
    "try:\n",
    "    # 首先尝试使用 wandb API 读取（如果运行已同步到云端）\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # 读取元数据以获取可能的 project 信息\n",
    "    metadata_file = os.path.join(wandb_run_dir, 'files', 'wandb-metadata.json')\n",
    "    run_id = 'h8hpdlth'\n",
    "    \n",
    "    # 尝试常见的 project 名称，或者从目录结构推断\n",
    "    # 通常 project 名称可以从运行目录的父目录推断\n",
    "    possible_projects = ['Search-R1', 'Search-R1-open', 'Search-R1-nq_hotpotqa_train']\n",
    "    \n",
    "    print(\"尝试使用 wandb API 读取历史数据...\")\n",
    "    \n",
    "    run_data = None\n",
    "    for project in possible_projects:\n",
    "        try:\n",
    "            # 尝试读取运行（如果已同步）\n",
    "            run_path = f\"{project}/{run_id}\"\n",
    "            run_data = api.run(run_path)\n",
    "            print(f\"成功从 project '{project}' 读取运行数据\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if run_data:\n",
    "        # 读取历史数据\n",
    "        history = run_data.history()\n",
    "        print(f\"历史数据形状: {history.shape}\")\n",
    "        print(f\"历史数据列: {list(history.columns)}\")\n",
    "        \n",
    "        # 查找所有 val/test_score/* 相关的列\n",
    "        val_test_score_cols = [col for col in history.columns if 'val/test_score' in col]\n",
    "        \n",
    "        if val_test_score_cols:\n",
    "            print(f\"\\n找到的 val/test_score 列: {val_test_score_cols}\")\n",
    "            \n",
    "            # 提取每一轮的数据\n",
    "            # 使用 _step 列作为轮次标识（如果存在）\n",
    "            step_col = '_step' if '_step' in history.columns else None\n",
    "            \n",
    "            for idx, row in history.iterrows():\n",
    "                # 使用 step 值作为轮次标识（如果存在），否则使用索引\n",
    "                if step_col and pd.notna(row[step_col]):\n",
    "                    round_key = f\"step_{int(row[step_col])}\"\n",
    "                else:\n",
    "                    round_key = f\"step_{idx}\"\n",
    "                \n",
    "                if round_key not in val_test_scores_by_round:\n",
    "                    val_test_scores_by_round[round_key] = {}\n",
    "                \n",
    "                for col in val_test_score_cols:\n",
    "                    if pd.notna(row[col]):\n",
    "                        # 从列名中提取 data_source\n",
    "                        # 格式: val/test_score/{data_source}\n",
    "                        data_source = col.replace('val/test_score/', '')\n",
    "                        if data_source not in val_test_scores_by_round[round_key]:\n",
    "                            val_test_scores_by_round[round_key][data_source] = []\n",
    "                        val_test_scores_by_round[round_key][data_source].append(float(row[col]))\n",
    "        else:\n",
    "            print(\"历史数据中未找到 val/test_score 列\")\n",
    "            print(\"可用的指标列:\")\n",
    "            for col in history.columns[:20]:  # 显示前20个列\n",
    "                print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(\"运行未同步到云端，将尝试读取本地文件...\")\n",
    "        \n",
    "        # 尝试读取本地 wandb 文件\n",
    "        run_file = os.path.join(wandb_run_dir, 'run-mng4jg2z.wandb')\n",
    "        if os.path.exists(run_file):\n",
    "            print(f\"找到本地运行文件: {run_file}\")\n",
    "            print(\"注意：本地 .wandb 文件是二进制格式，需要使用 wandb SDK 解析\")\n",
    "            print(\"建议：使用 'wandb sync' 命令将运行同步到云端，然后使用 API 读取\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"使用 wandb API 读取失败: {e}\")\n",
    "    print(\"将尝试从日志文件读取...\")\n",
    "\n",
    "# 方法2: 使用 wandb 的命令行工具导出数据（如果可用）\n",
    "print(\"\\n尝试方法2：使用 wandb 命令行工具...\")\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    # 尝试使用 wandb 命令行工具导出历史数据\n",
    "    # wandb sync 可以将离线运行同步，但我们需要的是读取历史\n",
    "    \n",
    "    # 更直接的方法：使用 Python 直接解析 wandb 文件\n",
    "    # 但这比较复杂，因为 .wandb 是二进制格式\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"方法2失败: {e}\")\n",
    "\n",
    "# 方法3: 使用 wandb 的内部工具直接读取 .wandb 文件\n",
    "print(\"\\n尝试方法3：直接读取 wandb 历史文件...\")\n",
    "\n",
    "try:\n",
    "    # 使用 wandb 的内部工具读取历史数据\n",
    "    from wandb.sdk.internal.file_stream import FileStream\n",
    "    from wandb.sdk.lib import runid\n",
    "    \n",
    "    # wandb 的历史数据可能存储在多个地方\n",
    "    # 1. run-*.wandb 文件（二进制格式）\n",
    "    # 2. 如果已同步，可以通过 API 读取\n",
    "    \n",
    "    # 尝试使用 wandb 的内部 API 读取\n",
    "    # 这需要知道如何正确初始化\n",
    "    \n",
    "    # 更实用的方法：如果运行已同步，使用 API\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # 尝试通过运行目录读取（如果支持）\n",
    "    # 注意：wandb API 通常需要运行已同步到云端\n",
    "    \n",
    "    # 如果未同步，我们将依赖日志文件方法\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"方法3失败: {e}\")\n",
    "    print(\"将使用日志文件方法...\")\n",
    "\n",
    "# 方法4: 从日志文件中提取 val/test_score 数据（最可靠的方法）\n",
    "print(\"\\n方法4：从日志文件中提取 val/test_score 数据...\")\n",
    "\n",
    "log_files = list(Path(wandb_run_dir).glob('**/*.log'))\n",
    "print(f\"找到日志文件: {len(log_files)} 个\")\n",
    "\n",
    "# 存储每一轮的 val/test_score 数据\n",
    "# 格式: {round_number: {data_source: score}}\n",
    "round_scores = {}\n",
    "\n",
    "for log_file in log_files:\n",
    "    try:\n",
    "        print(f\"正在读取: {log_file.name}\")\n",
    "        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # 查找包含 val/test_score 的行\n",
    "        # 可能的格式：\n",
    "        # - val/test_score/nq: 0.xxx\n",
    "        # - {\"val/test_score/nq\": 0.xxx}\n",
    "        # - val/test_score/nq = 0.xxx\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            # 匹配各种可能的格式\n",
    "            patterns = [\n",
    "                r'val/test_score/(\\w+)[:\\s=]+([\\d.]+)',  # val/test_score/nq: 0.xxx\n",
    "                r'[\"\\']val/test_score/(\\w+)[\"\\']\\s*:\\s*([\\d.]+)',  # \"val/test_score/nq\": 0.xxx\n",
    "                r'val/test_score/(\\w+)\\s+([\\d.]+)',  # val/test_score/nq 0.xxx\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, line)\n",
    "                for data_source, score_str in matches:\n",
    "                    try:\n",
    "                        score = float(score_str)\n",
    "                        # 尝试推断轮次（从上下文或行号）\n",
    "                        # 如果日志中有 step 或 epoch 信息，可以提取\n",
    "                        round_key = None\n",
    "                        \n",
    "                        # 查找当前行附近的 step/epoch 信息\n",
    "                        context_start = max(0, i - 5)\n",
    "                        context_end = min(len(lines), i + 5)\n",
    "                        context = ' '.join(lines[context_start:context_end])\n",
    "                        \n",
    "                        # 尝试提取 step 或 epoch\n",
    "                        step_match = re.search(r'step[:\\s=]+(\\d+)', context, re.IGNORECASE)\n",
    "                        epoch_match = re.search(r'epoch[:\\s=]+(\\d+)', context, re.IGNORECASE)\n",
    "                        global_step_match = re.search(r'global[_\\s]?step[:\\s=]+(\\d+)', context, re.IGNORECASE)\n",
    "                        \n",
    "                        if global_step_match:\n",
    "                            round_key = f\"step_{global_step_match.group(1)}\"\n",
    "                        elif step_match:\n",
    "                            round_key = f\"step_{step_match.group(1)}\"\n",
    "                        elif epoch_match:\n",
    "                            round_key = f\"epoch_{epoch_match.group(1)}\"\n",
    "                        else:\n",
    "                            # 如果没有找到明确的轮次信息，使用行号作为标识\n",
    "                            round_key = f\"round_{i}\"\n",
    "                        \n",
    "                        if round_key not in round_scores:\n",
    "                            round_scores[round_key] = {}\n",
    "                        if data_source not in round_scores[round_key]:\n",
    "                            round_scores[round_key][data_source] = []\n",
    "                        round_scores[round_key][data_source].append(score)\n",
    "                        \n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"读取日志文件 {log_file} 失败: {e}\")\n",
    "\n",
    "# 合并从 wandb API 和日志文件读取的数据\n",
    "if val_test_scores_by_round:\n",
    "    # 如果从 wandb API 读取到了数据，使用它\n",
    "    round_scores = val_test_scores_by_round\n",
    "    print(\"\\n使用从 wandb API 读取的数据\")\n",
    "elif not round_scores:\n",
    "    # 如果都没有数据，尝试使用 round_scores（从日志文件读取的）\n",
    "    pass\n",
    "\n",
    "# 整理数据并计算每一轮的平均值\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"每一轮所有 data_source 的平均值:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if round_scores:\n",
    "    round_averages = {}  # {round_key: 该轮所有 data_source 的平均值}\n",
    "    \n",
    "    for round_key, scores_dict in sorted(round_scores.items()):\n",
    "        # 计算每个 data_source 在该轮的平均值\n",
    "        round_data_source_avgs = []  # 存储该轮每个 data_source 的平均值\n",
    "        \n",
    "        for data_source, scores in scores_dict.items():\n",
    "            avg_score = np.mean(scores)\n",
    "            round_data_source_avgs.append(avg_score)\n",
    "        \n",
    "        # 计算该轮所有 data_source 的平均值\n",
    "        if round_data_source_avgs:\n",
    "            round_avg = np.mean(round_data_source_avgs)\n",
    "            round_averages[round_key] = round_avg\n",
    "    \n",
    "    # 显示每一轮的平均值\n",
    "    if round_averages:\n",
    "        df_round_avgs = pd.DataFrame([\n",
    "            {'round': round_key, 'average_score': avg_score}\n",
    "            for round_key, avg_score in sorted(round_averages.items())\n",
    "        ])\n",
    "        print(df_round_avgs.to_string(index=False))\n",
    "        \n",
    "        # 计算所有轮次平均值的总体平均\n",
    "        overall_round_avg = np.mean(list(round_averages.values()))\n",
    "        print(f\"\\n所有轮次平均值的总体平均: {overall_round_avg:.4f}\")\n",
    "    else:\n",
    "        print(\"未找到有效数据\")\n",
    "        \n",
    "else:\n",
    "    print(\"未找到 val/test_score 数据\")\n",
    "    print(\"\\n提示：\")\n",
    "    print(\"1. 确保运行已经记录了 val/test_score 指标\")\n",
    "    print(\"2. 如果运行已同步到 wandb 云端，可以使用 wandb API 读取\")\n",
    "    print(\"3. 检查日志文件中是否有相关记录\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bb30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searchr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
